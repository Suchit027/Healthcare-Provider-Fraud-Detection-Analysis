{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, p1, p2, p3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(31, 70, bias= True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(70, 70, bias= True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(70),\n",
    "            nn.Dropout(p = p1),\n",
    "            nn.Linear(70, 65, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(65, 60, bias= True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 60, bias= True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(60),\n",
    "            nn.Dropout(p= p2),\n",
    "            nn.Linear(60, 55, bias= True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(55, 50, bias= True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 45, bias= True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(45),\n",
    "            nn.Dropout(p= p3),\n",
    "            nn.Linear(45, 40, bias= True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 35, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(35, 25, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 20, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 15, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15, 10, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 5, bias = True)\n",
    "        )\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Linear(5, 1, bias= True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classify(self.features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x = torch.tensor(x.values, dtype= torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype= torch.float32)\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.x[index], self.y[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ytrain, xtest, ytest = pd.read_csv('./X_train.csv', header= None, index_col = None), pd.read_csv('./y_train.csv', header = None, index_col = None), pd.read_csv('./X_test.csv', header = None, index_col = None), pd.read_csv('./y_test.csv', header = None, index_col = None)\n",
    "train_data = Data(x= xtrain, y= ytrain)\n",
    "test_data = Data(x= xtest, y= ytest)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle= True)\n",
    "test_loader = DataLoader(test_data, batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (features): Sequential(\n",
       "    (0): Linear(in_features=31, out_features=70, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=70, out_features=70, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=70, out_features=65, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=65, out_features=60, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=60, out_features=60, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): Dropout(p=0.4, inplace=False)\n",
       "    (14): Linear(in_features=60, out_features=55, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=55, out_features=50, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=50, out_features=45, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): BatchNorm1d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): Dropout(p=0.5, inplace=False)\n",
       "    (22): Linear(in_features=45, out_features=40, bias=True)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=40, out_features=35, bias=True)\n",
       "    (25): ReLU()\n",
       "    (26): Linear(in_features=35, out_features=25, bias=True)\n",
       "    (27): ReLU()\n",
       "    (28): Linear(in_features=25, out_features=20, bias=True)\n",
       "    (29): ReLU()\n",
       "    (30): Linear(in_features=20, out_features=15, bias=True)\n",
       "    (31): ReLU()\n",
       "    (32): Linear(in_features=15, out_features=10, bias=True)\n",
       "    (33): ReLU()\n",
       "    (34): Linear(in_features=10, out_features=5, bias=True)\n",
       "  )\n",
       "  (classify): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network(0.3, 0.4, 0.5)\n",
    "checkpoint = torch.load('./best_parameters.pt', weights_only= False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "# learning rate decreased by power of 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.000001, weight_decay= 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 0 loss - 2843.3656286895275\n",
      "Epoch - 1 loss - 2839.092820852995\n",
      "Epoch - 2 loss - 2840.3029324114323\n",
      "Epoch - 3 loss - 2835.9831232130527\n",
      "Epoch - 4 loss - 2839.465943723917\n",
      "Epoch - 5 loss - 2837.273266404867\n",
      "Epoch - 6 loss - 2839.254652440548\n",
      "Epoch - 7 loss - 2838.962654441595\n",
      "Epoch - 8 loss - 2839.808662176132\n",
      "Epoch - 9 loss - 2838.771314203739\n",
      "Epoch - 10 loss - 2837.083151817322\n",
      "Epoch - 11 loss - 2837.336702644825\n",
      "Epoch - 12 loss - 2838.1909616589546\n",
      "Epoch - 13 loss - 2838.6478576362133\n",
      "Epoch - 14 loss - 2836.574931740761\n",
      "Epoch - 15 loss - 2838.1840093433857\n",
      "Epoch - 16 loss - 2837.5860404372215\n",
      "Epoch - 17 loss - 2837.8780706226826\n",
      "Epoch - 18 loss - 2836.7674401402473\n",
      "Epoch - 19 loss - 2836.3095359504223\n",
      "Epoch - 20 loss - 2837.939996123314\n",
      "Epoch - 21 loss - 2837.4796514213085\n",
      "Epoch - 22 loss - 2835.7667397856712\n",
      "Epoch - 23 loss - 2837.1573871970177\n",
      "Epoch - 24 loss - 2837.908105969429\n",
      "Epoch - 25 loss - 2837.1780201494694\n",
      "Epoch - 26 loss - 2838.099575340748\n",
      "Epoch - 27 loss - 2838.3953991234303\n",
      "Epoch - 28 loss - 2836.2603145241737\n",
      "Epoch - 29 loss - 2838.3693456351757\n",
      "Epoch - 30 loss - 2836.092309206724\n",
      "Epoch - 31 loss - 2838.0780154168606\n",
      "Epoch - 32 loss - 2837.1032474040985\n",
      "Epoch - 33 loss - 2837.6823943555355\n",
      "Epoch - 34 loss - 2834.9983735382557\n",
      "Epoch - 35 loss - 2837.174925804138\n",
      "Epoch - 36 loss - 2834.506697833538\n",
      "Epoch - 37 loss - 2837.817301362753\n",
      "Epoch - 38 loss - 2837.1717813014984\n",
      "Epoch - 39 loss - 2836.240131765604\n",
      "Epoch - 40 loss - 2837.2921720445156\n",
      "Epoch - 41 loss - 2837.1228436529636\n",
      "Epoch - 42 loss - 2836.4229255616665\n",
      "Epoch - 43 loss - 2836.1839011907578\n",
      "Epoch - 44 loss - 2838.144883811474\n",
      "Epoch - 45 loss - 2835.5971941947937\n",
      "Epoch - 46 loss - 2839.5118142068386\n",
      "Epoch - 47 loss - 2837.8881817162037\n",
      "Epoch - 48 loss - 2836.630630135536\n",
      "Epoch - 49 loss - 2837.313636213541\n",
      "Epoch - 50 loss - 2837.2452887892723\n",
      "Epoch - 51 loss - 2836.456316113472\n",
      "Epoch - 52 loss - 2838.091707319021\n",
      "Epoch - 53 loss - 2834.5890015661716\n",
      "Epoch - 54 loss - 2837.043622761965\n",
      "Epoch - 55 loss - 2836.414882391691\n",
      "Epoch - 56 loss - 2835.1382772922516\n",
      "Epoch - 57 loss - 2836.336833626032\n",
      "Epoch - 58 loss - 2836.782611757517\n",
      "Epoch - 59 loss - 2834.984311670065\n",
      "Epoch - 60 loss - 2836.1654838323593\n",
      "Epoch - 61 loss - 2836.0554056465626\n",
      "Epoch - 62 loss - 2837.9821439385414\n",
      "Epoch - 63 loss - 2836.8017878830433\n",
      "Epoch - 64 loss - 2836.165088504553\n",
      "Epoch - 65 loss - 2838.4772199094296\n",
      "Epoch - 66 loss - 2835.1977956295013\n",
      "Epoch - 67 loss - 2838.3278505802155\n",
      "Epoch - 68 loss - 2837.1513098478317\n",
      "Epoch - 69 loss - 2836.333824276924\n",
      "Epoch - 70 loss - 2835.5049141049385\n",
      "Epoch - 71 loss - 2837.5733963549137\n",
      "Epoch - 72 loss - 2836.1457701325417\n",
      "Epoch - 73 loss - 2835.6360949873924\n",
      "Epoch - 74 loss - 2834.5523349642754\n",
      "Epoch - 75 loss - 2836.859695672989\n",
      "Epoch - 76 loss - 2836.813641279936\n",
      "Epoch - 77 loss - 2839.1785586476326\n",
      "Epoch - 78 loss - 2838.736051350832\n",
      "Epoch - 79 loss - 2835.985996723175\n",
      "Epoch - 80 loss - 2836.0643103420734\n",
      "Epoch - 81 loss - 2835.018508076668\n",
      "Epoch - 82 loss - 2837.9450305104256\n",
      "Epoch - 83 loss - 2833.402868449688\n",
      "Epoch - 84 loss - 2836.243844270706\n",
      "Epoch - 85 loss - 2835.9937330782413\n",
      "Epoch - 86 loss - 2838.5676424503326\n",
      "Epoch - 87 loss - 2836.68654024601\n",
      "Epoch - 88 loss - 2837.838310599327\n",
      "Epoch - 89 loss - 2838.0469673871994\n",
      "Epoch - 90 loss - 2835.0983466506004\n",
      "Epoch - 91 loss - 2836.4461475014687\n",
      "Epoch - 92 loss - 2837.301711052656\n",
      "Epoch - 93 loss - 2836.0011483728886\n",
      "Epoch - 94 loss - 2834.0850509107113\n",
      "Epoch - 95 loss - 2835.7558867037296\n",
      "Epoch - 96 loss - 2837.144672781229\n",
      "Epoch - 97 loss - 2838.0749695301056\n",
      "Epoch - 98 loss - 2836.206731081009\n",
      "Epoch - 99 loss - 2837.504124313593\n",
      "Epoch - 100 loss - 2836.7389353513718\n",
      "Epoch - 101 loss - 2836.5178142786026\n",
      "Epoch - 102 loss - 2834.798696219921\n",
      "Epoch - 103 loss - 2836.392642110586\n",
      "Epoch - 104 loss - 2837.1015880405903\n",
      "Epoch - 105 loss - 2838.085924297571\n",
      "Epoch - 106 loss - 2836.9678365290165\n",
      "Epoch - 107 loss - 2834.9333265423775\n",
      "Epoch - 108 loss - 2836.563107728958\n",
      "Epoch - 109 loss - 2835.7741043567657\n",
      "Epoch - 110 loss - 2837.6218180656433\n",
      "Epoch - 111 loss - 2832.7396512031555\n",
      "Epoch - 112 loss - 2836.731755375862\n",
      "Epoch - 113 loss - 2836.48234257102\n",
      "Epoch - 114 loss - 2832.686200916767\n",
      "Epoch - 115 loss - 2836.8604433834553\n",
      "Epoch - 116 loss - 2835.820435255766\n",
      "Epoch - 117 loss - 2837.3354774415493\n",
      "Epoch - 118 loss - 2838.5215147435665\n",
      "Epoch - 119 loss - 2835.44044983387\n",
      "Epoch - 120 loss - 2838.222952693701\n",
      "Epoch - 121 loss - 2838.332686394453\n",
      "Epoch - 122 loss - 2836.566884458065\n",
      "Epoch - 123 loss - 2837.51751101017\n",
      "Epoch - 124 loss - 2835.2245586812496\n",
      "Epoch - 125 loss - 2836.275444716215\n",
      "Epoch - 126 loss - 2833.5620259940624\n",
      "Epoch - 127 loss - 2835.0267164111137\n",
      "Epoch - 128 loss - 2837.8765624165535\n",
      "Epoch - 129 loss - 2837.5253271460533\n",
      "Epoch - 130 loss - 2836.277767598629\n",
      "Epoch - 131 loss - 2835.7725881040096\n",
      "Epoch - 132 loss - 2833.721532344818\n",
      "Epoch - 133 loss - 2835.0897759199142\n",
      "Epoch - 134 loss - 2835.33710950613\n",
      "Epoch - 135 loss - 2838.0535859167576\n",
      "Epoch - 136 loss - 2835.9678244292736\n",
      "Epoch - 137 loss - 2835.8444360792637\n",
      "Epoch - 138 loss - 2836.17033880949\n",
      "Epoch - 139 loss - 2835.619191855192\n",
      "Epoch - 140 loss - 2836.2313200831413\n",
      "Epoch - 141 loss - 2835.643236130476\n",
      "Epoch - 142 loss - 2835.181052982807\n",
      "Epoch - 143 loss - 2834.9672709703445\n",
      "Epoch - 144 loss - 2835.1147633194923\n",
      "Epoch - 145 loss - 2836.51790907979\n",
      "Epoch - 146 loss - 2836.3017147779465\n",
      "Epoch - 147 loss - 2836.9722928106785\n",
      "Epoch - 148 loss - 2835.5024234056473\n",
      "Epoch - 149 loss - 2834.6192864775658\n",
      "Epoch - 150 loss - 2837.9494879841805\n",
      "Epoch - 151 loss - 2836.0570963025093\n",
      "Epoch - 152 loss - 2836.862100094557\n",
      "Epoch - 153 loss - 2838.5852088034153\n",
      "Epoch - 154 loss - 2835.0657416582108\n",
      "Epoch - 155 loss - 2836.314105808735\n",
      "Epoch - 156 loss - 2835.5227347016335\n",
      "Epoch - 157 loss - 2836.0502472519875\n",
      "Epoch - 158 loss - 2836.915664702654\n",
      "Epoch - 159 loss - 2834.4598118960857\n",
      "Epoch - 160 loss - 2836.7298447191715\n",
      "Epoch - 161 loss - 2836.2130493819714\n",
      "Epoch - 162 loss - 2839.122145086527\n",
      "Epoch - 163 loss - 2836.9867883622646\n",
      "Epoch - 164 loss - 2834.756604641676\n",
      "Epoch - 165 loss - 2835.874737173319\n",
      "Epoch - 166 loss - 2838.1248121857643\n",
      "Epoch - 167 loss - 2835.2373602092266\n",
      "Epoch - 168 loss - 2835.8138851821423\n",
      "Epoch - 169 loss - 2834.9611498117447\n",
      "Epoch - 170 loss - 2834.749702811241\n",
      "Epoch - 171 loss - 2835.8091083467007\n",
      "Epoch - 172 loss - 2835.8557120263577\n",
      "Epoch - 173 loss - 2837.988633722067\n",
      "Epoch - 174 loss - 2840.248309791088\n",
      "Epoch - 175 loss - 2839.1157752275467\n",
      "Epoch - 176 loss - 2838.2570091784\n",
      "Epoch - 177 loss - 2837.7928851544857\n",
      "Epoch - 178 loss - 2834.384824693203\n",
      "Epoch - 179 loss - 2834.606597214937\n",
      "Epoch - 180 loss - 2834.3832118809223\n",
      "Epoch - 181 loss - 2838.257731437683\n",
      "Epoch - 182 loss - 2837.2236819267273\n",
      "Epoch - 183 loss - 2836.228635162115\n",
      "Epoch - 184 loss - 2834.6797163188457\n",
      "Epoch - 185 loss - 2834.871982783079\n",
      "Epoch - 186 loss - 2836.7371598780155\n",
      "Epoch - 187 loss - 2836.138574063778\n",
      "Epoch - 188 loss - 2836.1877325475216\n",
      "Epoch - 189 loss - 2835.7070260345936\n",
      "Epoch - 190 loss - 2836.994943857193\n",
      "Epoch - 191 loss - 2836.643532216549\n",
      "Epoch - 192 loss - 2835.115333765745\n",
      "Epoch - 193 loss - 2835.8988007605076\n",
      "Epoch - 194 loss - 2836.9959039092064\n",
      "Epoch - 195 loss - 2835.908660084009\n",
      "Epoch - 196 loss - 2834.2077997922897\n",
      "Epoch - 197 loss - 2837.8134407401085\n",
      "Epoch - 198 loss - 2837.978839457035\n",
      "Epoch - 199 loss - 2835.8833517432213\n"
     ]
    }
   ],
   "source": [
    "loss_list_train, loss_list_val = [], []\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    running_loss_trian, running_loss_val = 0.0, 0.0\n",
    "    for input, target in train_loader:\n",
    "        input, target = input.to('cuda'), target.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss_trian += loss.item()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, target in test_loader:\n",
    "            input, target = input.to('cuda'), target.to('cuda')\n",
    "            output = torch.round(model(input))\n",
    "            loss = criterion(output, target)\n",
    "            running_loss_val += loss.item()\n",
    "    loss_list_train.append(running_loss_trian / len(train_loader))\n",
    "    loss_list_val.append(running_loss_val / len(test_loader))\n",
    "    print(f'Epoch - {epoch} loss - {running_loss_trian}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "# plt.plot(range(len(loss_list_train)), loss_list_train, color= 'red')\n",
    "plt.plot(range(len(loss_list_val)), loss_list_val, color = 'blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "# plt.plot(range(len(loss_list_train)), loss_list_train, color= 'red')\n",
    "plt.plot(range(len(loss_list_val)), loss_list_val, color = 'blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "# plt.plot(range(len(loss_list_train)), loss_list_train, color= 'red')\n",
    "plt.plot(range(len(loss_list_val)), loss_list_val, color = 'blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "# plt.plot(range(len(loss_list_train)), loss_list_train, color= 'red')\n",
    "plt.plot(range(len(loss_list_val)), loss_list_val, color = 'blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7208806563720397\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for input, target in test_loader:\n",
    "        input, target = input.to('cuda'), target.to('cuda')\n",
    "        output = torch.round(model(input))\n",
    "        all_preds.extend(output.to('cpu'))\n",
    "        all_labels.extend(target.to('cpu'))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(all_preds, all_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'accuracy': '72%', 'epochs_run': 1500}\n",
    "torch.save(checkpoint, './best_parameters.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
